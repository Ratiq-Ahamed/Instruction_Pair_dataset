{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvcFOvTG3qiA"
      },
      "outputs": [],
      "source": [
        "!pip install ctransformers[cuda]\n",
        "!wget https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/resolve/main/llama-2-7b-chat.ggmlv3.q4_0.bin"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import spacy\n",
        "from ctransformers import AutoModelForCausalLM\n",
        "from google.colab import files\n",
        "from difflib import SequenceMatcher\n",
        "import glob\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "def similarity(a, b):\n",
        "    return SequenceMatcher(None, a, b).ratio()\n",
        "\n",
        "def process_chunk(llm, chunk, num_pairs=10, max_new_tokens=2000):\n",
        "    try:\n",
        "        prompt = f\"\"\"[INST]You are an AI assistant specializing in creating training data for ISMO Bio-photonic, a company in the biophotonics industry. Based on the following information about ISMO Bio-photonic:\n",
        "\n",
        "{chunk}\n",
        "\n",
        "Create {num_pairs} diverse, high-quality, and detailed instruction-output pairs about ISMO Bio-photonic. If specific information is not provided, generate plausible and relevant content that would be typical for a biophotonics company. Ensure variety in the types of questions and information covered.\n",
        "\n",
        "Consider the following aspects:\n",
        "1. Company background and history\n",
        "2. Products and services\n",
        "3. Technology and innovation\n",
        "4. Applications of their technology\n",
        "5. Company leadership and team\n",
        "6. Industry impact and achievements\n",
        "7. Future goals and projects\n",
        "8. Customer support and service\n",
        "9. Partnerships and collaborations\n",
        "10. Company values and mission\n",
        "11. Company CEO\n",
        "Format each pair as follows:\n",
        "### Instruction: [A specific question or instruction about ISMO Bio-photonic]\n",
        "### Output: [A detailed, informative response that could be used to train an AI about the company]\n",
        "\n",
        "Make sure each pair is unique and non-redundant. If you need to extrapolate beyond the given information, ensure it aligns with the company's focus on biophotonics.[/INST]\"\"\"\n",
        "\n",
        "        response = llm(prompt, max_new_tokens=max_new_tokens)\n",
        "        if isinstance(response, str):\n",
        "            pairs = re.split(r'(?m)^### Instruction:', response)[1:]\n",
        "            results = []\n",
        "            for pair in pairs:\n",
        "                parts = re.split(r'(?m)^### Output:', pair)\n",
        "                if len(parts) == 2:\n",
        "                    instruction = parts[0].strip()\n",
        "                    output = parts[1].strip()\n",
        "                    results.append({\n",
        "                        \"instruction\": instruction,\n",
        "                        \"input\": \"\",\n",
        "                        \"output\": output\n",
        "                    })\n",
        "            return results\n",
        "        else:\n",
        "            print(f\"Unexpected response type: {type(response)}\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing chunk: {e}\")\n",
        "        return None\n",
        "\n",
        "def create_meaningful_chunks(text, nlp, max_chunk_length=1000):\n",
        "    doc = nlp(text)\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    current_length = 0\n",
        "\n",
        "    for sent in doc.sents:\n",
        "        if current_length + len(sent) > max_chunk_length and current_chunk:\n",
        "            chunks.append(' '.join([str(s) for s in current_chunk]))\n",
        "            current_chunk = []\n",
        "            current_length = 0\n",
        "\n",
        "        current_chunk.append(sent)\n",
        "        current_length += len(sent)\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(' '.join([str(s) for s in current_chunk]))\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def filter_redundant_pairs(pairs, similarity_threshold=0.6):\n",
        "    filtered_pairs = []\n",
        "    for pair in pairs:\n",
        "        is_redundant = False\n",
        "        for existing_pair in filtered_pairs:\n",
        "            if similarity(pair['instruction'], existing_pair['instruction']) > similarity_threshold or \\\n",
        "               similarity(pair['output'], existing_pair['output']) > similarity_threshold:\n",
        "                is_redundant = True\n",
        "                break\n",
        "        if not is_redundant:\n",
        "            filtered_pairs.append(pair)\n",
        "    return filtered_pairs\n",
        "\n",
        "def process_files(input_directory, output_file_path, model_path, target_pairs=250):\n",
        "    try:\n",
        "        llm = AutoModelForCausalLM.from_pretrained(\n",
        "            model_path,\n",
        "            model_type=\"llama\",\n",
        "            lib=\"cuda\",\n",
        "            gpu_layers=35,\n",
        "            threads=2,\n",
        "            context_length=2048\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        print(\"Falling back to CPU...\")\n",
        "        try:\n",
        "            llm = AutoModelForCausalLM.from_pretrained(\n",
        "                model_path,\n",
        "                model_type=\"llama\",\n",
        "                lib=\"avx2\",\n",
        "                gpu_layers=0,\n",
        "                context_length=2048\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model on CPU: {e}\")\n",
        "            return\n",
        "\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "    all_results = []\n",
        "    file_count = 0\n",
        "\n",
        "    files = glob.glob(os.path.join(input_directory, '*.txt'))\n",
        "    for filename in tqdm(files, desc=\"Processing files\", unit=\"file\"):\n",
        "        file_count += 1\n",
        "        print(f\"\\nProcessing file {file_count}: {filename}\")\n",
        "\n",
        "        with open(filename, 'r', encoding='utf-8') as file:\n",
        "            content = file.read()\n",
        "\n",
        "        chunks = create_meaningful_chunks(content, nlp)\n",
        "\n",
        "        for i, chunk in enumerate(tqdm(chunks, desc=\"Processing chunks\", unit=\"chunk\", leave=False)):\n",
        "            if len(all_results) >= target_pairs:\n",
        "                break\n",
        "\n",
        "            processed_chunk = process_chunk(llm, chunk, num_pairs=5)\n",
        "            if processed_chunk:\n",
        "                filtered_chunk = filter_redundant_pairs(processed_chunk)\n",
        "                all_results.extend(filtered_chunk)\n",
        "                print(f\"Total pairs generated: {len(all_results)}\")\n",
        "            else:\n",
        "                print(f\"Error processing chunk {i+1} in file {filename}: No output generated.\")\n",
        "\n",
        "        if len(all_results) >= target_pairs:\n",
        "            break\n",
        "\n",
        "    # Trim to target number of pairs if exceeded\n",
        "    all_results = all_results[:target_pairs]\n",
        "\n",
        "    with open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
        "        json.dump(all_results, outfile, indent=4)\n",
        "\n",
        "    print(f\"Generated {len(all_results)} instruction-output pairs about ISMO Bio-photonic.\")\n",
        "    print(f\"Processed data saved to {output_file_path}\")\n",
        "\n",
        "# Set paths\n",
        "model_path = \"/content/llama-2-7b-chat.ggmlv3.q4_0.bin\"  # Replace with your model path\n",
        "input_directory = \"/content/input_texts\"  # Directory containing input text files\n",
        "output_file_path = \"/content/ismo_instruction_data.json\"\n",
        "\n",
        "# Create input directory and upload files\n",
        "!mkdir -p {input_directory}\n",
        "uploaded = files.upload()\n",
        "for filename, content in uploaded.items():\n",
        "    with open(os.path.join(input_directory, filename), 'wb') as f:\n",
        "        f.write(content)\n",
        "\n",
        "# Process the files\n",
        "try:\n",
        "    process_files(input_directory, output_file_path, model_path, target_pairs=150)\n",
        "    print(f\"Saved processed data (instruction format) about ISMO Bio-photonic to {output_file_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error processing files: {e}\")"
      ],
      "metadata": {
        "id": "cdcP8ZBn3sOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dFgVr2bT3sMh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}